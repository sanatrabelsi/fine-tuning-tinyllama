import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling
from datasets import Dataset, load_dataset
from peft import get_peft_model, LoraConfig
import pandas as pd
from huggingface_hub import login, create_repo, upload_folder
import json
from peft import PeftModel, PeftConfig
# Define paths and model IDs
model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
output_model_dir = "tinyllama13"
json_file_path = "/content/dataset.json"  # Path to your JSON file

# Define function to get model and tokenizer
def get_model_and_tokenizer(model_id):
    # Load the tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    tokenizer.pad_token = tokenizer.eos_token

    # Load model
    model = AutoModelForCausalLM.from_pretrained(model_id)

    if hasattr(model, 'device_map'):
        model = model.to_empty()

    model.config.use_cache = False
    model.config.pretraining_tp = 1
    return model, tokenizer

# Load model and tokenizer
model, tokenizer = get_model_and_tokenizer(model_id)

# Define the prompt to test
prompt = "En tant qu'utilisateur, je veux pouvoir me connecter à mon compte, afin de pouvoir accéder à mes données personnelles. Génère des cas de test pour cette user story."

# Test the pre-trained model
def generate_response(prompt, model, tokenizer):
    input_text = f"Vous êtes un assistant compétent en développement et test de logiciels..</s>User: {prompt}</s>Assistant:"
    inputs = tokenizer(input_text, return_tensors="pt", truncation=True, max_length=512)
    input_ids = inputs['input_ids'].to(model.device)
    attention_mask = inputs['attention_mask'].to(model.device)
    outputs = model.generate(
        input_ids=input_ids,
        attention_mask=attention_mask,
        max_new_tokens=150,
        do_sample=True,
        temperature=0.5,
        top_k=30,
        top_p=0.8
    )
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    response = response.split("Assistant:")[-1].strip()
    return response.strip()

print("Before fine-tuning:")
print(generate_response(prompt, model, tokenizer))

# Define function to format training data
def formatted_train(system, prompt, response) -> str:
    return f"{system}</s>{prompt}</s>{response}</s>"

# Load and prepare training data from JSON file
def prepare_train_data(json_file_path):
    with open(json_file_path, 'r') as file:
        data_list = json.load(file)

    df = pd.DataFrame(data_list)
    df["text"] = df.apply(lambda x: formatted_train(x["System"], x["Prompt"], x["Response"]), axis=1)
    dataset = Dataset.from_pandas(df)
    return dataset

# Define LoRA configuration
peft_config = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

# Prepare training data
data = prepare_train_data(json_file_path)

# Tokenize the data
def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=512)

train_dataset = data.map(tokenize_function, batched=True)
train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])

# Define training arguments
training_arguments = TrainingArguments(
    output_dir=output_model_dir,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,
    optim="adamw_torch",
    learning_rate=2e-4,
    lr_scheduler_type="cosine",
    save_strategy="epoch",
    logging_steps=10,
    num_train_epochs=3,
    max_steps=250,
    fp16=True
)

# Initialize PEFT model
peft_model = get_peft_model(model, peft_config)

# Ensure model is on the GPU
if torch.cuda.is_available():
    peft_model = peft_model.to('cuda')
else:
    raise RuntimeError("CUDA is not available. Please check your GPU setup.")

# Initialize Trainer with PEFT model
trainer = Trainer(
    model=peft_model,
    args=training_arguments,
    train_dataset=train_dataset,
    tokenizer=tokenizer,
    data_collator=DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False
    )
)

# Start training
torch.cuda.empty_cache()  # Clear GPU memory
trainer.train()

# Save the fine-tuned model
peft_model.save_pretrained(output_model_dir)
tokenizer.save_pretrained(output_model_dir)

# Generate and save the model configuration
from transformers import AutoConfig
config = AutoConfig.from_pretrained(model_id)
config.save_pretrained(output_model_dir)

# Login to Hugging Face
login(token='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')

# Create a repo on Hugging Face
create_repo('sanattt/tinyllama13', private=True)

# Push model and tokenizer to Hugging Face
upload_folder(
    repo_id='sanattt/tinyllama13',
    folder_path=output_model_dir,
    path_in_repo='',
    token='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'
)

# Load the fine-tuned model and tokenizer
fine_tuned_model = AutoModelForCausalLM.from_pretrained(output_model_dir)
fine_tuned_tokenizer = AutoTokenizer.from_pretrained(output_model_dir)

# Ensure the fine-tuned model is on GPU
if torch.cuda.is_available():
    fine_tuned_model = fine_tuned_model.to('cuda')
else:
    raise RuntimeError("CUDA is not available. Please check your GPU setup.")

# Test the fine-tuned model
def generate_fine_tuned_response(prompt, model, tokenizer):
    input_text = f"Vous êtes un assistant IA serviable...</s>User: {prompt}</s>Assistant:"
    inputs = tokenizer(input_text, return_tensors="pt", truncation=True, max_length=512)
    input_ids = inputs['input_ids'].to(model.device)
    attention_mask = inputs['attention_mask'].to(model.device)
    outputs = model.generate(
        input_ids=input_ids,
        attention_mask=attention_mask,
        max_new_tokens=650,
        do_sample=True,
        temperature=0.5,
        top_k=30,
        top_p=0.8
    )
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    response = response.split("Assistant:")[-1].strip()
    return response.strip()

print("After fine-tuning:")
print(generate_fine_tuned_response(prompt, fine_tuned_model, fine_tuned_tokenizer))
